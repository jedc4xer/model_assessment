{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785f7df4-3f73-4a61-87c1-f4588e820142",
   "metadata": {},
   "source": [
    "## This example is recreated from the towards datascience tutorial that is saved as a pdf in the github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdb1be6-10fb-4cfd-bb48-1ba610afd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rain = pd.read_csv(\"data/weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f01372-ca7a-4d4c-8817-fdaedd959d86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378151b1-5896-4a3a-94ac-2926c1d4221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"Date\", \"Location\", \"RainTomorrow\", \"Rainfall\"]\n",
    "\n",
    "rain.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d816e-51e4-484c-8090-406d9f949c8d",
   "metadata": {},
   "source": [
    "If the proportion is higher than 40% we will drop the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8651fbee-2883-4505-8c5b-5e5814473840",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_props = rain.isna().mean(axis=0)\n",
    "over_threshold = missing_props[missing_props >= 0.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf3152-3796-478c-a3bd-b557752f4bfb",
   "metadata": {},
   "source": [
    "Three columns contain more than 40% missing values. We will drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b18e86-9e2f-4913-827c-a8e485e00431",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain.drop(over_threshold.index, \n",
    "          axis=1, \n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99722d45-c404-4df0-a7ef-affc4ef92cab",
   "metadata": {},
   "source": [
    "Now, before we move on to pipelines, let’s divide the data into feature and target arrays beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff3a0f9-d08d-4a90-93c2-6ecfe11ac655",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rain.drop(\"RainToday\", axis=1)\n",
    "y = rain.RainToday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989bb69-ca19-444e-8a35-ed7d83495279",
   "metadata": {},
   "source": [
    "Next, there are both categorical and numeric features. We will build two separate pipelines and combine them later.\n",
    "The next code examples will heavily use Sklearn-Pipelines. If you are not familiar with them, check out my separate article for the complete guide on them.\n",
    "For the categorical features, we will impute the missing values with the mode of the column and encode them with One-Hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77a10a6-8215-4667-82d5-25efe548f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c172b8-3bfd-4b82-a4d2-d95cdb6a8fbb",
   "metadata": {},
   "source": [
    "For the numeric features, I will choose the mean as an imputer and StandardScaler so that the features have 0 mean and a variance of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559bdb68-8e10-47ce-9afc-d492debc1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72eb14-7e8e-4360-8f30-f3011c98b785",
   "metadata": {},
   "source": [
    "Finally, we will combine the two pipelines with a column transformer. To specify which columns the pipelines are designed for, we should first isolate the categorical and numeric feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc55e119-0f57-4c09-b3aa-4cdaa3e230fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "num_cols = X.select_dtypes(include=\"number\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54930d-98a2-4422-bca4-0adbea745d50",
   "metadata": {},
   "source": [
    "Next, we will input these along with their corresponding pipelines into a ColumnTransFormer instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6329051-54e1-42c7-9380-33caa2601ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a1bff-774a-4528-a761-b772ef74bd54",
   "metadata": {},
   "source": [
    "The full pipeline is finally ready. The only thing missing is the XGBoost classifier, which we will add in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ab568f-8f87-4c78-b4d2-e4c8de22edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e8540a-7608-4750-85be-8aae34ae7dfa",
   "metadata": {},
   "source": [
    "Fortunately, the classifier follows the familiar fit-predict pattern of sklearn meaning we can freely use it as any sklearn model.\n",
    "Before we train the classifier, let’s preprocess the data and divide it into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66877d3b-45f5-48a9-9aeb-5f50aa65ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "X_processed = full_processor.fit_transform(X)\n",
    "y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    y.values.reshape(-1, 1)\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, stratify=y_processed, random_state=1121218\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341e16a-8bef-44f4-bcac-47716cac2c3e",
   "metadata": {},
   "source": [
    "Since the target contains NaN, I imputed it by hand. Also, it is important to pass y_processed to stratify so that the split contains the same proportion of categories in both sets.\n",
    "Now, we fit the classifier with default parameters and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83fe64fb-c38c-4f26-9677-13054dbef008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Work\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:29:01] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8507080984463082"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955a529-bab2-4432-997a-2db7de839c66",
   "metadata": {},
   "source": [
    "### Check out this resource deeper as it goes into detailed discussion about hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c2e9e-70d2-49a9-9a1a-b9443e25da56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67ce0517-2ca7-42e2-b502-ee8d92f745e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_depth\": [3, 4, 5, 7],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.05],\n",
    "    \"gamma\": [0, 0.25, 1],\n",
    "    \"reg_lambda\": [0, 1, 10],\n",
    "    \"scale_pos_weight\": [1, 3, 5],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97621106-6e09-4476-b858-82539e12c83b",
   "metadata": {},
   "source": [
    "In the grid, I fixed subsample and colsample_bytree to recommended values to speed things up and prevent overfitting.\n",
    "We will import GridSearchCV from sklearn.model_selection, instantiate and fit it to our preprocessed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10a134-9815-433e-9e56-551c3ab6af62",
   "metadata": {},
   "source": [
    "**This step below can take 10 - 20 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e559c0-bed9-4073-85ea-fa64a8f37e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Init classifier\n",
    "# xgb_cl = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "# # Init Grid Search\n",
    "# grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring=\"roc_auc\")\n",
    "\n",
    "# # Fit\n",
    "# _ = grid_cv.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1125b-3bb4-4414-bcc8-2a7b8d13a636",
   "metadata": {},
   "source": [
    "After an excruciatingly long time, we finally got the best params and best score:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2582ce8-a10d-4d79-a13b-8d364f59eb05",
   "metadata": {},
   "source": [
    "**Expect the step above to take about 10-20 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afffc1f-49bd-4a89-b1c0-0079b9757b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9191f71-82df-4824-abaf-388521293432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be824976-8488-4c89-9393-7a7657addcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### There is a little more after this point, but I stopped here for the night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb99ad-1605-44e6-b1d8-59612207f5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
